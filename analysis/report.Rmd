---
title: "clustereval: evaluating graph based clustering"
output: html_notebook
---

# Introduction

  Community detection, a type of clustering, is a common step in the analysis of computational graphs and networks. In community detection, the nodes of a graph are divided into different partitions; this is achieved by optimizing a modularity function. Modularity is roughly a measure of how connected a specific partition of a graph is connected to itself vs the rest of the graph.

This is the basic formula for modularity
$$ Q = \frac{1}{2m}\sum_{c} (M_c - \frac{K_c^2}{4m})  $$
where *M_c* is the number of internal edges in a patition  *c* ( edge strictly between nodes in *c*),
*K_c^2* is the total number of edges associated with nodes in *c*, and *m* is the total number of edges in the graph. Therefore, an ideal community *c* where Q=1 would only have connections to itself and be completely disconnected to the rest of the graph. For most algorithmns that optimize modularity (louvain, leiden, etc), the number of partitions, analgous to the number of clusters in conventional clustering, is deterministic, when using this definition of modularity 

  A more commonly used version of modularity(Reichardt and Bornholdt's model) is 
$$ Q = \frac{1}{2m}\sum_{c} (M_c - \gamma\frac{K_c^2}{4m})  $$ 
where $$ \gamma $$ is the resolution parameter, which can tune the numbern of partitions detected by the partitioning algorithmn. Just like in convential clustering, the choice of how many partitions to generate is a non-trivial task. As there is often no ground truth in determing the true number of clusters, it is difficult to say which set of labels identified by clustering are the most accurate.


## Graph partitioning of a k-Nearest Neighbor graph
k-NN graphs built from approximating nearest neighbors is a key step in many types of data analyses, but particularly for scRNA-seq. For each observation, the top K closest cells are identified; This can be converted to graph form by making each observationa  node, and having edges between nearest neighbors. When building a graph in this method, the number of edges directly depends on k; m = k * n_observations
  
## Metrics for determining clustering/partitioning accuracy.
  There have been multiple metrics proposed for evaluating clustering accuracy. When ground truth labels exist, label based methods like the Rand index or normalized Mutual information work well. However, most of the time this is not the case. Ground-truth free methods for evaluating clustering accuracy like the silhouette coefficient or Daviesâ€“Bouldin index are often distance based, but distance is not as easily defined for graphs. 
  von Luxburg et al. proposes a more general approach for evaluating clustering: the "perturbation experiment". In a perturbation experiment, some amount of noise is added to a subset of the data; this perturbed data is then re-clustered. This is repeated multiple times, and then the original, non-perturbed dataset is compared against the peturbed ones.  This experiment can be repeated across different hyperparameters for a clustering algorithmn. The extent to which a particular clustering remains accurate over a set of perturbations is its immutability(I), or more formally, 
  $$ I(hp, n) = \frac1n\sum_{n=0} d(C, C_n) $$
where *hp* represents the set of hyperparamters used to generate non-perturbed clustering *C*,
*n* = number of pertrubations 
*C_n* is clustering from perturbed data, 
and *d(...)* is a method for measuring the distance between two sets of clusters. 

In this project, graphs can be perturbed by  randomly adding and removing edges, or randomly changing edge weights.

Perturbation experiments are done across multiple sets of hyperparameters, and the set of HP which maximize *I* are the "best" hyper parameters. Note that this procedure is really answering the questions "How sensitve is a proposed set of labels to small changes in the input data"

### Measuring the distance between two sets of clusters 
  Any label based measure of accuracy can be used, ie those mentioned above. However, for this project, we consider two metrics previously described by Shekar et al., Stability(*S*) and Purity(*P*), which adapt label-based metrics specifcally for use in pertubation experiments.
  
#### Stability
Stability S calculated per-cluster for the non-perturbed data and based on shannon entropy. Keep in mind that the set of observations will be the same across non-perturbed and perturbed data, but perturbed data has permuted features.
$$ S_k = 1- \frac{1}{n}\sum_{i=1}^n \frac{H_i^k}{H_i^{Tot}} $$
where *i* is a perturbed dataset, *n* is the total number of rounds of perturbation, and *k* is a single cluster within the non-perturbed dataset.

H is the shannon entropy of a set of labels for data : $$ H = -\sum_c^C f_c * ln(f_c) $$, where:
*f_c* is the fraction of observations in a dataset that have label *c*, and *C* is the total set of labels for this dataset. In this above stability equation, $$H_i^{Tot}$$ is the shannon entropy for a perturbed dataset *i*. For $$H_i^k$$, the  perturbed dataset *i* subset to contain only the observations assigned to label *k* in the non-perturbed dataset, and then the shannon entropy is calculated for this subset. *S_k* is the average of $$ \frac{H_i^k}{H_i^{Tot}}$$ across all perturbations. And so each cluster in the non-perturbed labels will ha ve its own S_k 

#### Purity 

Let O be the cluster j in perturbation experiment i that has the highest overlap with non-perturbed cluster k.  $$ O_j =   max_{j=1}( NonPerturbed_k \in Perturbed_j) $$ 

Purity is then

$$ P_k = \frac{1}{n} \sum_{i=1}^{n} \frac{len(O_j)}{len({Perturbed_j})} $$

 Purity is the the fraction of observations that in a reference cluster k that maximally overlaps with a cluster perturbation experiment. 
 
 
 
### An Example 

```{r include=F, echo = F}
library(tidyverse)
library(patchwork)
n =100
df <- bind_rows(
    tibble(x= rnorm(n, 5), y=rnorm(n, 5), clustering_1 = 'A'),
    tibble(x= rnorm(n, -5), y=rnorm(n, 5), clustering_1 = 'B'),
    tibble(x= rnorm(n, -5), y=rnorm(n, -5), clustering_1 = 'C')

)

lab_df = tibble(x =c(-5,5, -5, 5), y=c(5, 5, -5, -5), lab = c('1', '2', '3', '4'))
main_clu <- ggplot(df) + 
    geom_point(aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df[1:3,], aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('reference')+
    theme_minimal()

merge_df <- df
merge_df[merge_df$clustering_1 == 'C',]$x <- rnorm(n, 5)
merge_df[merge_df$clustering_1 == 'C',]$y <- rnorm(n, 5)

merge_clu <- ggplot() + 
    geom_point( data = merge_df, aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df[1:2,], aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('Stable, but not Pure')+
    theme_minimal()


split_df <- df 
split_df$x[251:300] <- rnorm(50, 5)
split_df$y[251:300] <- rnorm(50, -5)
split_df$clustering_2A <- c(rep('1', 100),rep('2', 100), rep('3', 50), rep('4', 50) )
split_clu <- ggplot(split_df) + 
    geom_point(aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df, aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('Pure, but not Stable')+
    theme_minimal()


prom_df <-df[c(1:74, 100:174,200:275),  ] %>% 
    mutate(clustering_2B =c(rep('1', 75),rep('2', 75), rep('3', 75) ) )
prom_df <- bind_rows(prom_df, 
                     tibble(x=rnorm(75, 5), y= rnorm(75, -5),
                     clustering_1 = sample(c('A','B','C'), 75,  replace = T), clustering_2B = '4' )
                    )
prom_clu <- ggplot(prom_df) + 
    geom_point(aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df, aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('Not pure not Stable')+
    theme_minimal()
```

Here is a plot that describes the behavior of each metric:

```{r, echo =F}
(main_clu + merge_clu) / ( split_clu | prom_clu) +plot_layout(guides='collect' ) & plot_annotation(tag_levels = 'A') & theme_bw() & 
    #geom_vline(xintercept = 0)& geom_hline(yintercept = 0) & 
    xlim(c(-10, 10)) & ylim(c(-10,10))  & guides(color = guide_legend(title = 'Original Cluster'))


```


  In this example, Cluster 3 of the reference cluster is compared in different situations. In A vs B, cluster 3 has been merged with another cluster. The Stability for A-3  is 1, because all observations in A-3 are preserved within a single cluster, B-2. However, the purity of A-3 is low, because the overall size of the maximum overlap cluster, B-2, is larger than the original cluster A-3. Data that has High Stability is likely over clustered, as smaller cluster become merged into another cluster when only a few edges are added/removed

The opposite is true for Panel C. Cluster A-3 splits into two separate clusters, C-2, C-3. Data with high Purity but low stability is likely under-clustered, as the addition/subtraction of a few edges is enough to cause the cluster to split. 
 
 
 
 
 
 
 