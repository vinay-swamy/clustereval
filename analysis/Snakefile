import numpy as np

RES_RANGE= [.2, .4, .6, .8, 1.0,  1.2,  1.4, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]
rule all:
    input:
        clustering = expand('cluster_out/ds-{countMat}_alg-{alg}_knn-{k}_.csv.gz', k=list(range(4,101,2)),
                                countMat = ['sanes_amacrine','pbmc'], alg = ['louvain', 'leiden']),
        metrics = expand('cluster_metrics/ds-{countMat}_alg-{alg}_knn-{k}_.csv.gz', k=list(range(4,101,2)),
                                countMat = ['sanes_amacrine','pbmc'], alg = ['louvain', 'leiden'])

rule make_count_matrices:
    output:
        'data/sanes_amacrine_preproccessed.csv.gz',
        'data/pbmc_preproccessed.csv.gz',
    shell:
        '''

        wget https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz
        wget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE149nnn/GSE149715/suppl/GSE149715%5FMouseAC%5Fcount%5Fmatrix%2Ecsv%2Egz
        tar -xzf pbmc3k_filtered_gene_bc_matrices.tar.gz
        module load R/4.0.3
        Rscript scripts/make_raw_data.R
        rm pbmc3k_filtered_gene_bc_matrices.tar.gz
        rm filtered_gene_bc_matrices/ -rf
        rm GSE149715_MouseAC_count_matrix.csv.gz

        '''


rule run_parameter_exp_clustering:
    input: 
        mat = 'data/{countMat}_preproccessed.csv.gz'
    output:
        clustering = 'cluster_out/ds-{countMat}_alg-{alg}_knn-{k}_.csv.gz',
        metrics = 'cluster_metrics/ds-{countMat}_alg-{alg}_knn-{k}_.csv.gz'

    run:
        import clustereval as ce
        import numpy as np 
        import pandas as pd
        import leidenalg 
        import louvain 

        reduction = pd.read_csv(input.mat, index_col=0)
        clu_obj = ce.cluster.ClusterExperiment(reduction, 0)
        clu_obj.buildNeighborGraph(int(wildcards.k), 'l2', 150, False, False, None, None)
        if wildcards.alg == 'louvain':
            labels = clu_obj.run_louvain(
                vertex_partition_method=louvain.RBConfigurationVertexPartition,
                resolution=1.0,
                jac_weighted_edges='weight'
            )
        elif wildcards.alg == 'leiden':
            labels = clu_obj.run_leiden(
                vertex_partition_method=leidenalg.RBConfigurationVertexPartition,
                resolution = 1.0,
                n_iter=5,
                jac_weighted_edges='weight'
            )
        
        labels_clean = clu_obj.merge_singletons(labels, 50)
        ref_labels = pd.DataFrame().assign(Barcode = list(reduction.index), labels = labels_clean).sort_values('labels')
        ref_labels.to_csv(output.clustering)

        perturbed_labels = ce.cluster.run_perturbations(clu_obj, 1 , wildcards.alg, 50, .05, None, 50, 0)
        
        metrics = ce.metrics.calculate_metrics(ref_labels, perturbed_labels)
        metrics.to_csv(output.metrics)



# rule calculate_parameter_exp_metrics:
#     input: 
#         expand('par_exp_out/{{countMat}}-exp_alg-{{alg}}_res-{res}_knn-{n}_.csv.gz', 
#                 res = [.2,.3, .4, .5, .6, .7, .8, .9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0], n=list(range(5,100,5)))
#     output:
#         results = 'par_exp_metrics/{countMat}-exp_alg-{alg}.pickle'
#     params:
#         glob_string= lambda wildcards: f'par_exp_out/{wildcards.countMat}-exp_alg-{wildcards.alg}*_.csv.gz'
#     run:
#         import clustereval as ce
#         import numpy as np 
#         import pandas as pd
#         import pickle
#         exp_results = ce.calc_metrics.pairwise_metric_calculation_fromdisk(params.glob_string, 96)
#         with open(output.results, 'wb+') as outfile:
#             exp_results = [{'exp_param': i.exp_param, 
#                             'cluster_ids':i.cluster_ids, 
#                             'stability_scores': i.stability_scores, 
#                             'purity_scores': i.purity_scores} for i in exp_results  ]
#             pickle.dump(exp_results, outfile)
        
